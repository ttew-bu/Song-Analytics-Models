{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import re\r\n",
    "import nltk\r\n",
    "import matplotlib as mpl\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "import gensim\r\n",
    "from gensim.corpora import Dictionary\r\n",
    "from gensim.models import LdaMulticore\r\n",
    "from gensim.models import CoherenceModel\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.stem import PorterStemmer\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from collections import Counter\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.model_selection import RandomizedSearchCV\r\n",
    "from sklearn.dummy import DummyClassifier\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.svm import LinearSVC\r\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#Create a df based on the genres data collected in other steps and reduce it down to key columns\r\n",
    "df_genres = pd.read_csv('C:/Users/trist/Labelled_Data.csv')\r\n",
    "df_genres = df_genres.loc[:,['artist','track',\"cat_numbers\"]]\r\n",
    "\r\n",
    "#Create a df based on the lyrics data; do a similar reduction so that the lyrics and join keys are in the df\r\n",
    "df_lyrics = pd.read_csv('C:/Users/trist/OneDrive/Desktop/lyrics_new.csv')\r\n",
    "df_lyrics = df_lyrics.loc[:,['artist','track',\"lyrics\"]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#Merge the lyric and genre data based on the artist and track as keys using an inner join to only bring back matches\r\n",
    "df = pd.merge(df_genres,df_lyrics,on=['artist','track'],how='inner')\r\n",
    "\r\n",
    "#Remove the columns that have a null (this would be in the lyrics or genres section)\r\n",
    "df = df.dropna()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Potentially filter out the smaller categories until training data can be expanded to have even classes\r\n",
    "df = df[ (df['cat_numbers'] != 'Latin') & (df['cat_numbers'] != 'EDM') & (df['cat_numbers'] != 'Jazz')]\r\n",
    "\r\n",
    "#Check the counts of each genre in the dataset with lyrics\r\n",
    "df['cat_numbers'].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Rock       378\n",
       "Country    225\n",
       "Pop        194\n",
       "Rap        173\n",
       "R&B        158\n",
       "Name: cat_numbers, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Remove extraneous text (e.g. embedcopy) and verse indicators\r\n",
    "new_lyrics = []\r\n",
    "\r\n",
    "#Iterate through the songs\r\n",
    "for row in df['lyrics']:\r\n",
    "    \r\n",
    "    #Convert to a string\r\n",
    "    row = str(row)\r\n",
    "    \r\n",
    "    #remove the embedshare string placed in Genius lyrics\r\n",
    "    row = row.replace(\"EmbedShare URLCopyEmbedCopy\",\"\")\r\n",
    "    \r\n",
    "    #sub the bracket expressions via regex, these are commonly something like [(Shakira) Verse 1:] which are not lyrics\r\n",
    "    row = re.sub(r\"[\\[].*?[\\]]\", \"\", row)\r\n",
    "    \r\n",
    "    #sub line breaks out via regex\r\n",
    "    row = re.sub(r\"\\n+\", \" \", row)\r\n",
    "    \r\n",
    "    #sub all other punctuation out via regex\r\n",
    "    row = re.sub(r'[^\\w\\s]', '', row)\r\n",
    "    \r\n",
    "    #Convert all upper-case words to lower-case\r\n",
    "    row = row.lower()\r\n",
    "    \r\n",
    "    #add new lyrics to the list that will replace the original column\r\n",
    "    new_lyrics.append(row)\r\n",
    "\r\n",
    "#Replace the lyrics column with the cleaned lyrics\r\n",
    "df['lyrics'] = new_lyrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#Check out the df after the first round of manipulation and before vectorizing the words using tfidf\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              artist               track cat_numbers  \\\n",
       "0  Montgomery Gentry           Lucky Man     Country   \n",
       "1       Pretty Ricky      On The Hotline         Rap   \n",
       "2         Candlemass  Clouds Of Dementia        Rock   \n",
       "4   Billy Currington     I Got A Feelin'     Country   \n",
       "6       Alan Jackson   Where I Come From     Country   \n",
       "\n",
       "                                              lyrics  \n",
       "0   i have days where i hate my job this little t...  \n",
       "1   its five in the mornin and im up havin phone ...  \n",
       "2  jaded and demented in the attic the bonemen so...  \n",
       "4   i dont want to rush this thing i dont want to...  \n",
       "6   well i was rolling wheels and shifting gears ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>track</th>\n",
       "      <th>cat_numbers</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Montgomery Gentry</td>\n",
       "      <td>Lucky Man</td>\n",
       "      <td>Country</td>\n",
       "      <td>i have days where i hate my job this little t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pretty Ricky</td>\n",
       "      <td>On The Hotline</td>\n",
       "      <td>Rap</td>\n",
       "      <td>its five in the mornin and im up havin phone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Candlemass</td>\n",
       "      <td>Clouds Of Dementia</td>\n",
       "      <td>Rock</td>\n",
       "      <td>jaded and demented in the attic the bonemen so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Billy Currington</td>\n",
       "      <td>I Got A Feelin'</td>\n",
       "      <td>Country</td>\n",
       "      <td>i dont want to rush this thing i dont want to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alan Jackson</td>\n",
       "      <td>Where I Come From</td>\n",
       "      <td>Country</td>\n",
       "      <td>well i was rolling wheels and shifting gears ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "\r\n",
    "#Create a custom tokenizer so that we can stem or lemmatize words and see how that impacts performance (accuracy)\r\n",
    "def tokenize_with_stem(lyrics):\r\n",
    "    '''Take lyrics and tokenize them with a stem function e.g. PorterStemmer'''\r\n",
    "    #tokenize the lyrics\r\n",
    "    tokenized_lyrics = nltk.word_tokenize(lyrics)\r\n",
    "    \r\n",
    "    #Stem the lyrics after tokenization to get the roots of each token\r\n",
    "    stemmed_lyrics = [PorterStemmer().stem(lyric) for lyric in tokenized_lyrics]\r\n",
    "    return stemmed_lyrics\r\n",
    "\r\n",
    "#Combine stemming and lemming on stopwords to get dictionary def then boil down to stems in effort to reduce redundancy\r\n",
    "\r\n",
    "#Define stopwords via NLTK english corpus for stopwords\r\n",
    "stopwords = set(stopwords.words(\"english\"))\r\n",
    "\r\n",
    "#Stem stops down to their word stems based on root words\r\n",
    "stemmed_stops = [PorterStemmer().stem(word) for word in stopwords]\r\n",
    "\r\n",
    "\r\n",
    "#Set up the vectorizer using tfidf\r\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', \r\n",
    "                        encoding='utf-8', ngram_range=(1, 2), stop_words=stemmed_stops)\r\n",
    "\r\n",
    "#run the vectorizer to get the features to determine classification and put the outcomes in an array\r\n",
    "features = tfidf.fit_transform(df['lyrics']).toarray()\r\n",
    "\r\n",
    "#get our labels for classes\r\n",
    "labels = df['cat_numbers']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Now that we have fully preprocessed, train test split\r\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, labels, \r\n",
    "                                                    test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Create a list of models to loop through for performance at baseline level\r\n",
    "model_list = [DummyClassifier(strategy=\"stratified\"),\r\n",
    "              LogisticRegression(),RandomForestClassifier(random_state=42),\r\n",
    "              KNeighborsClassifier(n_neighbors=5),LinearSVC(random_state=42)]\r\n",
    "\r\n",
    "#Cross validate 5x\r\n",
    "cv = 5\r\n",
    "\r\n",
    "#Store list of tuples to create df\r\n",
    "tuples = []\r\n",
    "\r\n",
    "#Iterate through models and check for CV\r\n",
    "for m in model_list:\r\n",
    "    cv_acc = np.mean(cross_val_score(m,features,labels))\r\n",
    "    \r\n",
    "    \r\n",
    "    #fit\r\n",
    "    m.fit(X_train,Y_train)\r\n",
    "    \r\n",
    "    #predict\r\n",
    "    train_pred = m.predict(X_train)\r\n",
    "    test_pred = m.predict(X_test)\r\n",
    "    \r\n",
    "    #Accuracy test stored in values\r\n",
    "    test_acc = accuracy_score(test_pred,Y_test)\r\n",
    "    \r\n",
    "    tuples.append((m,test_acc,cv_acc))\r\n",
    "    \r\n",
    "tuples_df = pd.DataFrame(tuples,columns=[\"Model\",\"Test Accuracy\",\"CV Accuracy\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Select optimal model\r\n",
    "tuples_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Tune Hyperparameters for LogReg\r\n",
    "c_values = np.logspace(-1,5, 100)\r\n",
    "\r\n",
    "accuracies = []\r\n",
    "#Iterate through C_values\r\n",
    "for c in c_values:\r\n",
    "    \r\n",
    "    #Instantiate\r\n",
    "    logreg_model = LogisticRegression(C=c, max_iter=10000)\r\n",
    "    \r\n",
    "    #fit\r\n",
    "    logreg_model.fit(X_train,Y_train)\r\n",
    "    \r\n",
    "    #predict\r\n",
    "    train_pred = logreg_model.predict(X_train)\r\n",
    "    test_pred = logreg_model.predict(X_test)\r\n",
    "    \r\n",
    "    #Accuracy on train\r\n",
    "    train_acc = accuracy_score(train_pred,Y_train)\r\n",
    "    \r\n",
    "    #Accuracy on test\r\n",
    "    test_acc = accuracy_score(test_pred,Y_test)\r\n",
    "    \r\n",
    "    #Add pairing to list so convert to df to view results\r\n",
    "    accuracies.append((c,train_acc,test_acc))\r\n",
    "\r\n",
    "logreg_results = pd.DataFrame(accuracies, columns=['Regularization Parameter','Train Accuracy','Test_Accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "logreg_results['Test_Accuracy'].max()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for col in ['Train Accuracy','Test_Accuracy']:\r\n",
    "    plt.plot(logreg_results['Regularization Parameter'],logreg_results[col],label=col)\r\n",
    "    plt.legend()\r\n",
    "    plt.xscale('log')\r\n",
    "    plt.title('Accuracies Across Regularization Values')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Tune Hyperparameters for SVC\r\n",
    "c_values = np.logspace(-1,5, 100)\r\n",
    "\r\n",
    "accuracies = []\r\n",
    "#Iterate through C_values\r\n",
    "for c in c_values:\r\n",
    "    \r\n",
    "    #Instantiate\r\n",
    "    svc_model = LinearSVC(random_state=42,C=c,max_iter=10000)\r\n",
    "    \r\n",
    "    #fit\r\n",
    "    svc_model.fit(X_train,Y_train)\r\n",
    "    \r\n",
    "    #predict\r\n",
    "    train_pred = svc_model.predict(X_train)\r\n",
    "    test_pred = svc_model.predict(X_test)\r\n",
    "    \r\n",
    "    #Accuracy on train\r\n",
    "    train_acc = accuracy_score(train_pred,Y_train)\r\n",
    "    \r\n",
    "    #Accuracy on test\r\n",
    "    test_acc = accuracy_score(test_pred,Y_test)\r\n",
    "    \r\n",
    "    #Add pairing to list so convert to df to view results\r\n",
    "    accuracies.append((c,train_acc,test_acc))\r\n",
    "\r\n",
    "svc_results = pd.DataFrame(accuracies, columns=['Regularization Parameter','Train Accuracy','Test_Accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "svc_results[svc_results['Test_Accuracy']==svc_results['Test_Accuracy'].max()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for col in ['Train Accuracy','Test_Accuracy']:\r\n",
    "    plt.plot(svc_results['Regularization Parameter'],svc_results[col],label=col)\r\n",
    "    plt.legend()\r\n",
    "    plt.xscale('log')\r\n",
    "    plt.title('Accuracies Across Regularization Values')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Tune Hyperparameters for KNN\r\n",
    "neighbors = [x for x in range(1,50)]\r\n",
    "\r\n",
    "accuracies = []\r\n",
    "#Iterate through C_values\r\n",
    "for n in neighbors:\r\n",
    "    \r\n",
    "    #Instantiate\r\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=n)\r\n",
    "    \r\n",
    "    #fit\r\n",
    "    knn_model.fit(X_train,Y_train)\r\n",
    "    \r\n",
    "    #predict\r\n",
    "    train_pred = knn_model.predict(X_train)\r\n",
    "    test_pred = knn_model.predict(X_test)\r\n",
    "    \r\n",
    "    #Accuracy on train\r\n",
    "    train_acc = accuracy_score(train_pred,Y_train)\r\n",
    "    \r\n",
    "    #Accuracy on test\r\n",
    "    test_acc = accuracy_score(test_pred,Y_test)\r\n",
    "    \r\n",
    "    #Add pairing to list so convert to df to view results\r\n",
    "    accuracies.append((n,train_acc,test_acc))\r\n",
    "\r\n",
    "knn_results = pd.DataFrame(accuracies, columns=['Regularization Parameter','Train Accuracy','Test_Accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for col in ['Train Accuracy','Test_Accuracy']:\r\n",
    "    plt.plot(knn_results['Regularization Parameter'],knn_results[col],label=col)\r\n",
    "    plt.legend()\r\n",
    "    plt.title('Accuracies Across Regularization Values')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "knn_results['Test_Accuracy'].max()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Random Forest cross val by checking out randomsearchCV\r\n",
    "\r\n",
    "accuracies = []\r\n",
    "\r\n",
    "depths = [x for x in range(1,501,5)]\r\n",
    "trees = [x for x in range(1,1001,5)]\r\n",
    "\r\n",
    "param_grid = {\r\n",
    "    'max_depth': depths,\r\n",
    "    'n_estimators': trees}\r\n",
    "\r\n",
    "\r\n",
    "random_forest_model = RandomizedSearchCV(RandomForestClassifier(random_state=42), param_grid, scoring='accuracy',random_state=42)\r\n",
    "\r\n",
    "#Fit \r\n",
    "random_forest_model.fit(X_train,Y_train)\r\n",
    "\r\n",
    "#Predict\r\n",
    "rf_train_preds = random_forest_model.predict(X_train)\r\n",
    "rf_test_preds = random_forest_model.predict(X_test)\r\n",
    "\r\n",
    "#Report Out\r\n",
    "rf_test_acc = accuracy_score(rf_test_preds,Y_test)\r\n",
    "rf_train_acc = accuracy_score(rf_train_preds,Y_train)\r\n",
    "\r\n",
    "accuracies.append((random_forest_model,rf_train_acc,rf_test_acc))\r\n",
    "    \r\n",
    "rf_df = pd.DataFrame(accuracies, columns=['Model','Train Accuracy','Test_Accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf_df['Test_Accuracy'].max()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "confusion_matrix(rf_test_preds,Y_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#MVP model here\n",
    "c_max = 0.151991\n",
    "\n",
    "model = LinearSVC(random_state=42,C=c_max,max_iter=10000).fit(X_train,Y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import pickle\n",
    "#Pickle the model so it can be sent to production\n",
    "pickle.dump (model, open('genre_pred_model.pickle', 'wb'))\n",
    "\n",
    "#Also pickle the vectorizer since it is needed to process the data\n",
    "pickle.dump (tfidf.fit(df['lyrics']),open('vectorizer.pickle', 'wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "357d08fc6475d5f3d7f5382ddf4b1a0cd6014611561001adf4b40c4ba2d37830"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}